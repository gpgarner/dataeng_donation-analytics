Comments about the submitted data analysis script ‘donation-analytics.py’
can be found within the python script as well.

The following programs and dependencies were used when approaching this
“donation-analytics” challenge for my application to be a Data Engineering
fellow at Insight.

Python
Pandas
sys
numpy
heapq

I decided to load the data using Pandas so that I could easily clean the
file of the rows and information that would not be useful in completing
the challenge.

My cleaning procedure using Pandas followed the steps below:

1. 	Load the data using pandas, identifying the columns of interest. 
	Making sure to load the identification codes as strings in case of 
	leading-0 identifiers (zip codes and transaction dates were the most
	pertinent fields).
2. 	Provide the columns of interest in the loaded data frame with labels
	to allow for easier handling.
3.	Remove all rows where “Other ID” was not Null.
4. 	Remove all rows where one of the columns containing the ‘CMTE_ID’, 
	'NAME', 'ZIP_CODE', 'TRANSACTION_DT', or  ‘TRANSACTION_AMT' were null.
5. 	I then removed any rows with negative values in the ‘TRANSACTION_AMT’
	as I considered these rows to be indicative of returned funds, and
	I only considered the actual donated amount in this analysis.
6. 	Keep only the first five digits of the zip code, so that all the zip
	codes are of uniform length.
7.	Save the 4 digit year in its own column, as this piece of information
	from the date was most relevant.
8. 	Reformat the ‘TRANSACTION_DT’ column into mm/dd/yyyy format, so that
	the column would be more easily sorted (to take care of any rows that
	were out of chronological order).
7.	Sort the information by date.
8. 	I noticed when working with an ‘itcont.txt’ file from the FEC website
	provided, that there were many instances in the data where the date
	was input incorrectly, giving dates that were nonsensical. Therefore,
	I removed any dates within the database that were not in the years
	1980-2019, as the archive files only go back to 1980.
9. 	I then only kept rows that consisted of a duplication of a previous
	combination of ‘Name’ and ‘ZIP_CODE’, so that I only considered
	repeat donors.

In order to setup my analysis to be prepared for the desired calculations,
I used Pandas to provide me with a list of all the unique zip codes, 
recipients, and years that could be found in the dataset of interest.

This allowed me to create two arrays that I could use to keep running sums.
One for the number of donations given to a specific recipient in a specific
zip code. The second for the total amount donated to a specific recipient
in a specific zip code. Since I already sorted the data chronologically,
I could easily reset these areas to a point where all values are zero
whenever I begin investigating a new donation year, as I worked with the
stream of data.

To keep track of the running percentiles, I created a dictionary of each
unique zip code, where each entry had three parts: two unique heaps and
a percentile calculation. I tried to utilize the heaps to increase the
computational speed (however, the strain on the memory may be high due
to the dictionary approach) for the calculation of the nearest rank
percentile. The heaps are reset to empty whenever the data transitions
to a new year.

I then read through all the kept rows in the data set of interest, reporting:

1.	CMTE_ID of the current row’s recipient

2.	ZIP_CODE of current  row’s repeat donor

3.	YEAR of the current row’s donation

4.	running PERCENTILE cutoff calculation of repeat donors’ donations of the
	current row’s zip code in the current row’s year

5.	TOTAL_AMOUNT_DONATED to the current row’s recipient from repeat donors
	in the current row’s zip code in the current row’s year 

6.	TOTAL_REPEAT_DONORS to the current row’s recipient from the current
	row’s zip code in the current row’s year
	
All the values are separated by a "|" in the output file titled 'repeat_donors.txt'.
